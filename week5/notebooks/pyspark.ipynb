{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import col, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 13:29:43 WARN Utils: Your hostname, ante-ThinkPad-T15-Gen-2i resolves to a loopback address: 127.0.1.1; using 192.168.6.93 instead (on interface wlp0s20f3)\n",
      "23/11/14 13:29:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 13:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8de204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-14 13:29:44--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-01.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.161.127.198, 3.161.127.221, 3.161.127.152, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.161.127.198|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11886281 (11M) [binary/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2021-01.parquet.1’\n",
      "\n",
      "fhv_tripdata_2021-0 100%[===================>]  11,33M  10,7MB/s    in 1,1s    \n",
      "\n",
      "2023-11-14 13:29:45 (10,7 MB/s) - ‘fhv_tripdata_2021-01.parquet.1’ saved [11886281/11886281]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a52087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32645 fhv_tripdata_2021-01.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931021a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropOff_datetime', TimestampNTZType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', IntegerType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('fhv_tripdata_2021-01.parquet')\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1b10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('fhv_tripdata_2021-01.parquet').head(1000).to_parquet('head.parquet', index= False)\n",
    "\n",
    "df_pandas = pd.read_parquet('head.parquet')\n",
    "df_pandas['pickup_datetime'] = df_pandas['pickup_datetime'].apply(lambda x: None if pd.isna(x) else x)\n",
    "df_pandas['dropOff_datetime'] = df_pandas['dropOff_datetime'].apply(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.DoubleType(), True),\n",
    "    types.StructField('DOLocationID', types.DoubleType(), True),\n",
    "    types.StructField('SR_Flag', types.IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94052ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .parquet('fhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c270d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)\n",
    "df = df.withColumn('SR_Flag', when(col('SR_Flag').isNull(), '').otherwise(col('SR_Flag').cast('string')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c74515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|SR_Flag|\n",
      "+-------+\n",
      "|       |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('SR_Flag').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7796c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/ante/Desktop/de-z/week5/fhvhv/2021/01 already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23136/361302406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fhvhv/2021/01/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/ante/Desktop/de-z/week5/fhvhv/2021/01 already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203b5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: double (nullable = true)\n",
      " |-- DOLocationID: double (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64172a47",
   "metadata": {},
   "source": [
    "SELECT * FROM df WHERE hvfhs_license_num =  HV0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24840a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab1ca44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|             null|              B02930|2021-01-11 09:14:37|2021-01-11 09:56:30|        null|        69.0|       |\n",
      "|             null|              B01437|2021-01-21 15:06:18|2021-01-21 15:13:59|        null|        10.0|       |\n",
      "|             null|              B02133|2021-01-29 14:17:03|2021-01-29 14:52:31|        null|        null|       |\n",
      "|             null|              B01667|2021-01-03 15:54:37|2021-01-03 16:05:09|        null|        92.0|       |\n",
      "|             null|              B01051|2021-01-24 13:34:46|2021-01-24 13:38:08|        null|       147.0|       |\n",
      "|             null|              B00887|2021-01-24 15:40:35|2021-01-24 16:25:02|        null|       265.0|       |\n",
      "|             null|              B01339|2021-01-29 20:03:17|2021-01-29 20:07:41|        null|        42.0|       |\n",
      "|             null|              B00856|2021-01-15 20:13:48|2021-01-15 20:24:45|        null|        89.0|       |\n",
      "|             null|              B01340|2021-01-28 11:13:38|2021-01-28 11:19:52|        null|       213.0|       |\n",
      "|             null|              B02849|2021-01-18 15:35:29|2021-01-18 15:44:08|        null|        17.0|       |\n",
      "|             null|              B03006|2021-01-25 10:31:07|2021-01-25 11:07:33|       131.0|        70.0|       |\n",
      "|             null|              B00900|2021-01-06 10:27:45|2021-01-06 10:49:04|        null|       225.0|       |\n",
      "|             null|              B00900|2021-01-15 15:54:35|2021-01-15 16:19:41|        null|       180.0|       |\n",
      "|             null|              B01445|2021-01-27 08:30:00|2021-01-27 08:38:00|        14.0|        14.0|       |\n",
      "|             null|              B01145|2021-01-26 12:18:17|2021-01-26 13:08:31|        null|       247.0|       |\n",
      "|             null|              B02674|2021-01-02 23:01:58|2021-01-02 23:20:27|       102.0|       260.0|       |\n",
      "|             null|              B00706|2021-01-18 10:01:27|2021-01-18 10:09:58|       206.0|       115.0|       |\n",
      "|             null|              B01469|2021-01-05 08:01:20|2021-01-05 08:12:46|        null|       197.0|       |\n",
      "|             null|              B00937|2021-01-16 20:38:35|2021-01-16 20:51:13|        null|       247.0|       |\n",
      "|             null|              B00856|2021-01-28 02:24:28|2021-01-28 02:29:05|        null|        35.0|       |\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d98c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3175419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bb5d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38f0465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/b72| 2021-01-11|  2021-01-11|        null|        69.0|\n",
      "|  a/59d| 2021-01-21|  2021-01-21|        null|        10.0|\n",
      "|  a/855| 2021-01-29|  2021-01-29|        null|        null|\n",
      "|  e/683| 2021-01-03|  2021-01-03|        null|        92.0|\n",
      "|  e/41b| 2021-01-24|  2021-01-24|        null|       147.0|\n",
      "|  e/377| 2021-01-24|  2021-01-24|        null|       265.0|\n",
      "|  e/53b| 2021-01-29|  2021-01-29|        null|        42.0|\n",
      "|  e/358| 2021-01-15|  2021-01-15|        null|        89.0|\n",
      "|  e/53c| 2021-01-28|  2021-01-28|        null|       213.0|\n",
      "|  s/b21| 2021-01-18|  2021-01-18|        null|        17.0|\n",
      "|  a/bbe| 2021-01-25|  2021-01-25|       131.0|        70.0|\n",
      "|  a/384| 2021-01-06|  2021-01-06|        null|       225.0|\n",
      "|  a/384| 2021-01-15|  2021-01-15|        null|       180.0|\n",
      "|  e/5a5| 2021-01-27|  2021-01-27|        14.0|        14.0|\n",
      "|  e/479| 2021-01-26|  2021-01-26|        null|       247.0|\n",
      "|  s/a72| 2021-01-02|  2021-01-02|       102.0|       260.0|\n",
      "|  e/2c2| 2021-01-18|  2021-01-18|       206.0|       115.0|\n",
      "|  e/5bd| 2021-01-05|  2021-01-05|        null|       197.0|\n",
      "|  e/3a9| 2021-01-16|  2021-01-16|        null|       247.0|\n",
      "|  e/358| 2021-01-28|  2021-01-28|        null|        35.0|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropOff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00921644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+------------+------------+\n",
      "|pickup_datetime|dropOff_datetime|PULocationID|DOLocationID|\n",
      "+---------------+----------------+------------+------------+\n",
      "+---------------+----------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropOff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV003').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
